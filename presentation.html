<!doctype html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>Workflow Engines Up and Running</title>

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/sky.css">

    <!-- Theme used for syntax highlighting of code -->
    <link rel="stylesheet" href="lib/css/zenburn.css">
    <style>
        #left {
            left: -8.33%;
            text-align: left;
            float: left;
            width: 50%;
            z-index: -10;
        }

        #right {
            left: -10%;
            top: 75px;
            float: right;
            text-align: left;
            width: 50%;
            z-index: -10;
        }

        .reveal pre code {
            font-size: 1.5em;
            line-height: 1.3;
        }

        .reveal pre {
            width: 120%;
            margin-left: -10%;
            height: 80%;
        }

        .reveal p { 
            text-align: left;
            font-size: 80%;
        }
    </style>
    <!-- Printing and PDF exports -->
    <script>
        var link = document.createElement('link');
        link.rel = 'stylesheet';
        link.type = 'text/css';
        link.href = window.location.search.match(/print-pdf/gi) ? 'css/print/pdf.css' : 'css/print/paper.css';
        document.getElementsByTagName('head')[0].appendChild(link);
    </script>
</head>
<body>
<div class="reveal">
    <div class="slides">
        <section>
            <h2>Go with the Flow: Intro to Apache Airflow</h2>
            <h4>Austin Hacker</h4>
            <aside class="notes">
            </aside>
        </section>
        <section>
            <h2>What this talk is:</h2>
            <p>
                <ul>
                    <li>Beginner Level</li><br>
                    <li>Cover basic usage, components, and some best practices</li><br>
                </ul>
            </p>
        </section>
        <section>
            <h3>What is Airflow?</h3>
            <ul>
                <li>Workflow Engine</li>
                <li>Originally developed by Maxime Beauchemin at Airbnb</li>
                <li>"Programmatically author, schedule and monitor data pipelines"</li>
                <li>Distributed and Extensible</li>
                <li>Not for data streaming</li>
            </ul>
            <aside class="notes">
It was developed originally by Maxime Beauchemin at Airbnb. History: started in 2014, brought to Airbnb github in 2015.<br>
Moved to Apache incubation in 2016.<br>
By Authoring DAGs using python you will get great flexibility. With that Airflow gives you many features such as rich UI and scheduling tools.<br>
As we will see was designed with multiple components that enable it to be very scalable and stable.<br>
This is not a data streaming solution (Spark streaming, storm), similar solutions are Pinball, Oozie, Azkaban, and Luigi.
            </aside>
        </section>
        <section>
            <h3>Agenda</h3>
            <ul>
                <li>Quick Demo</li>
                <li>Designing Workflows</li>
                <li>Airflow Components</li>
            </ul>
            <aside class="notes">
            </aside>
        </section>
        <Section>
            <h4>Airflow Demo</h4>
            <aside class="notes">
The main UI is on port 8080<br>
Stats about the DAG run, Code, Graph.<br>
Show details about a DAG that ran, code<br>
Run a DAG and show logs etc.<br>
Show the logs of a task that run.<br>
Show components of our Docker setup and how we will work (DAGs directory, /tmp/work directory).<br>
Expand on how each container is created.
Show the flower UI - but won't need it in this tutorial.
            </aside>
        </Section>
        <section>
            <img class="stretch" style="height: 609px; width: 512.77px;" src="img/example.png">
            <aside class="notes">
<b>Simplified and Generalized</b>
The idea for this tutorial came as Austin and I were looking at different solutions to ETL jobs/workflow engines that
to be utilized by our team.<br>
This is an example of a DAG, a directed a cyclic graph we have in production.<br>
The tools we will see give you the ability to construct these DAGs, monitor, recover from failure and manage dependency.<br>
This tutorial will give you initial hands on experience on using these tools<br>
            </aside>
        </section>
        <section>
            <h3>Literally programmatic</h3>
            <pre><code data-trim data-noescape>
                dag = DAG(dag_id="dump_db", schedule_interval="@daily")

                def setup_metadata():
                    return {'start_time': datetime.utcnow()}
                
                setup_task = PythonOperator(task_id="setup_metadata", 
                    python_callable=setup_metadata, dag=dag)
                ...
            </code></pre>
            <aside class="notes">
            </aside>
        </section>
        <section>
            <h3>DAGs</h3>
            <ul>
                <li>Directed Acyclic Graph</li>
                <li>Sets schedule</li>
                <li>Provides default task behaviors</li>
                <li>Captures run metadata useable by tasks</li>
            </ul>
        </section>
        <section>
            <pre><code data-trim data-noescape>
                dag = DAG(dag_id="dump_db", schedule_interval="@daily")

                def setup_metadata():
                    return {'start_time': datetime.utcnow()}
                
                setup_task = PythonOperator(task_id="setup_metadata", 
                    python_callable=setup_metadata, dag=dag)
                ...
            </code></pre>
            <aside class="notes">
            </aside>
        </section>
        <section>
            <h3>Operators</h3>
            <ul>
                <li>Blueprints for tasks</li>
                <li>PythonOperator runs an arbitrary python function</li>
                <li>Specialized operators: EmailOperator, SlackAPIOperator, etc.</li>
                <li>Sensors</li>
                <li>Custom operators</li>
            </ul>
        </section>
        <section>
            <pre><code data-trim data-noescape>
                from airflow.models import BaseOperator
                from airflow.utils.decorators import apply_defaults
            </code></pre>

            <pre><code data-trim data-noescape>
                class PyconOperator(BaseOperator):
                    @apply_defaults
                    def __init__(self, params,
                            *args, **kwargs):
                        self.params = params
                        super(PyconOperator, self).__init__(
                            *args, **kwargs)
                    def execute(self, context):
                        return "Pycon2018"
            </code></pre>
            <aside class="notes">
                This is the most basic way to build your custom operator. You will have a similar init and the magic<br>
                happens in the `execute`
                apply_defaults -     Function decorator that Looks for an argument named "default_args", and
                fills the unspecified arguments from it.<br>
                You don't need to worry much about it just know this is what makes your class into a real operator you
                can run inside of a DAG. Just inheriting from BaseOperator is not enough.
            </aside>
        </section>
        <section>
            <h3>Tasks</h3>
            <ul>
                <li>Individual units of work</li>
                <li>Run predefined code with given arguments</li>
                <li>Specifies how code should run</li>
                <li>Define dependencies</li>
            </ul>
        </section>
        <section>
            <pre><code data-trim data-noescape>
                ...
                for table in get_tables():
                    extract_task = PostgresExtractOperator(
                        task_id=f"extract_{table}", dag=dag, 
                        table_name=table
                    )
                    setup_task >> extract_task
                    ...
            </code></pre>
            <aside class="notes">
            </aside>
        </section>
        <Section>
            <h4>Airflow Components</h4>
            <ul>
                <li>UI</li>
                <li>MetaData Database(Postgres)</li>
                <li>Scheduler(Executor)</li>
                <li>Workers*</li>
            </ul>
            <aside class="notes">
Airflow UI is a Flask application that is used to display, monitor and control our DAG execution.<br>
Metadata database is used to store information about the current state of DAGs and tasks to facilitate UI presentation and scheduler operation<br>
The scheduler is a python process that runs and is used to schedule DAG and tasks runs.<br>
Executors: Local & sequential - mostly dev. Celery, Mesos, Dask - for scaling out and having workers. We will use celery to demonstrate it.
The worker actually execute the python code that was defined.
            </aside>
        </Section>
        <section>
            <h4>Xcom Push</h4>
            <pre><code data-trim data-noescape>
                def pass_data(ds, **kwargs):
                    kwargs['ti'].xcom_push(
                        key='value from pusher', 
                        value="my val")
            </code></pre>
            <pre><code data-trim data-noescape>
                def pass_data_from_return(ds, **kwargs):
                    return "my val"
            </code></pre>
        </section>
        <section>
            <h4>Xcom Pull</h4>
            <pre><code data-trim data-noescape>
                def pull_data(ds, **kwargs):
                    ti = kwargs['ti']
                    pulled_value_1 = ti.xcom_pull(
                        key="value_from_pusher", 
                        task_ids="pass_data")
                    pulled_value_2 = ti.xcom_pull(
                        key=None, 
                        task_ids="pass_data_from_return")
            </code></pre>
        </section>
        <section>
            <h4>Error Retries</h4>
            <pre><code data-trim data-noescape>
                default_args = {
                    'email_on_retry': False,
                    'retries': 2,
                    'retry_delay': timedelta(seconds=10),
                }
            </code></pre>
            <pre><code data-trim data-noescape>
                BashOperator(
                    task_id='sleep',
                    bash_command='sleep 5',
                    retries=5,
                    dag=dag)
            </code></pre>
            <aside class="notes">
                Sometimes they fail and will not recover but sometimes just a retry will help.<br>
                You can see here that Airflow has built in support for this. You can define retries globally or you can define them per Task<br>
                This makes your job much more stable and robust.
            </aside>
        </section>
        <section>
            <h4>Error Notifications</h4>
            <pre><code data-trim data-noescape>
                default_args = {
                    'email': ['pycon2018@example.com'],
                    'email_on_failure': True,
                    'on_failure_callback': notify_error,
                }
            </code></pre>

            <pre><code data-trim data-noescape>
                def notify_error(context):
                    # we can have more information here such as
                    # even a full URL to the task failure
                    message = "Failure executing task {} on DAG {}"
                        .format(str(context['task'].task_id),
                                str(context['dag'].dag_id))
                    print(message)
            </code></pre>
            <aside class="notes">
                Some tasks will fail in your jobs. As the amount of jobs grows you cannot monitor all of them<br>
                You will need a way to get notifications of these failures. The email notifications are built in<br>
                As you can see you can put your own completely custom logic to have some notifications of these errors.
            </aside>
        </section>
        <section>
            <h4>Unit Test - Operators</h4>
            <pre><code data-trim data-noescape>
                from airflow.models import DAG, TaskInstance
                from test_dag import PyconOperator
            </code></pre>

            <pre><code data-trim data-noescape>
                class TestPyconOperator(TestCase):
                    def test_execute(self):
                        dag = DAG(dag_id='pycon_dag',
                                  start_date=datetime.now())
                        task = PyconOperator(dag=dag, params={},
                                             task_id='first_task')
                        ti = TaskInstance(task=task,
                                          execution_date=datetime.now())
                        result = task.execute(ti.get_template_context())
                        self.assertEqual(result, "Pycon2018")
            </code></pre>
<aside class="notes">
You need to do some setup by creating a mock DAG and a task.<br>
But it does open the option that when your logic or your use case becomes complex where you need to create your own DAGs<br>
You have the ability to create good unit tests around them.
</aside>
        </section>
        <section>
            <h4>DAG Deployment</h4>
            <ul>
                <li>Synchronize across Components</li>
                <li>Have in version control</li>
            </ul>
<aside class="notes">
Scheduler, Web, Worker all need the same DAG directory. Deployment should be fast. We haven't seen issues but might be <br>
good idea to pause the scheduler.<br>
It is a good practice to have it in version control and deploy once tests passed.<br>
</aside>
        </section>
        <section>
            <h4>DAG Directory Structure</h4>
            <ul>
                <li>Subdirectories</li>
                <li>Separate logic from DAGs</li>
            </ul>
<aside class="notes">
Separate to subdirectory based on different parts of responsibility or systems.<br>
For easier maintenance logic should leave either in separate directory or even different package.<br>
You can put you custom operators in the same path as the DAGs or in plugins directory but then restart of the scheduler is needed.
</aside>
        </section>
        <section>
            <section>
                <h4>More Airflow features</h4>
                <aside class="notes">
                    These are some features that make Airflow stand out (I think) to other workflow engines.<br>
                </aside>
            </section>
            <section>
                <h4>Plugins</h4>
                <ul>
                    <li>Operators</li>
                    <li>Views(UI)</li>
                </ul>
                <aside class="notes">
                    Plugins give you the ability to extend the core Airflow functionality.<br>
                    We saw you can define your own operators, the right way to do it is part of a plugin<br>
                    A hooks is how Airflow interacts with external resources and you can define your own<br>
                    Macros are the variables we saw you can use in templates we saw<br>
                    Defining your own views gives you the ability to extend the Airflow UI and it is just a Flask view<br>
                </aside>
            </section>
            <section>
                <h4>Variables</h4>
                <pre><code data-trim data-noescape>
                    from airflow.models import Variable
                    foo = Variable.get("foo")
                    bar = Variable.get("bar", deserialize_json=True)
                </code></pre>
                <aside class="notes">
                    You define variables using web ui or cli. Those are constants you can reference in your task.<br>
                    One of the usages is testing.<br>
                    Connections
                </aside>
            </section>
        </section>
        <section>
            <h4>Airflow Summary</h4>
            <div id="left">
                <b></b>
                <ul>
                    <li>Many Features</li>
                    <li>Robust</li>
                    <li>Scalable</li>
                </ul>
            </div>
            <div id="right">
                <ul>
                    <li>DAGs on schedule</li>
                    <li>Limited on full DAG testing</li>
                </ul>
            </div>
        </section>
    </div>
</div>

<script src="lib/js/head.min.js"></script>
<script src="js/reveal.js"></script>

<script>
    // More info https://github.com/hakimel/reveal.js#configuration
    Reveal.initialize({
        history: true,

        // More info https://github.com/hakimel/reveal.js#dependencies
        dependencies: [
            {src: 'plugin/markdown/marked.js'},
            {src: 'plugin/markdown/markdown.js'},
            {src: 'plugin/notes/notes.js', async: true},
            {
                src: 'plugin/highlight/highlight.js', async: true, callback: function () {
                    hljs.initHighlightingOnLoad();
                }
            }
        ]
    });
</script>
</body>
</html>
